# GPT-Neo Chatbot
## GPT-Neo
GPT-Neo is an open-source language processing model, created by EleutherAI, that uses deep learning to generate human-like text. It stands for "Generative Pre-trained Transformer-Neo" and is an improved version of OpenAI's GPT-2 model. GPT-Neo has been trained on a massive amount of data and can generate coherent and realistic text on a wide range of topics, making it a powerful tool for tasks such as text generation, summarization, and translation. Its architecture is based on the transformer neural network model and it can be fine-tuned on specific tasks using transfer learning techniques. Since it is open-source, developers can easily build and customize their own models based on GPT-Neo, making it a highly versatile and accessible tool for natural language processing.
## Code description 
This is the main code file of a Flask application. It imports necessary packages including `torch`, `transformers`, `flask`, `atexit`, `zmq`, and `socket`.

The code sets up the environment and loads the GPT-Neo model using the `GPT2Tokenizer` and `GPTNeoForCausalLM` classes from the transformers package. The model is loaded with the pre-trained weights from the `EleutherAI/gpt-neo-2.7B` checkpoint.

It also defines the Flask application with the root route `@app.route('/')`, which returns the `index.html` file using the `render_template` function.

The `/qa` route uses the `POST` method to receive a user's question and return an answer generated by the GPT-Neo model. The get_answer function is responsible for generating an answer for the input question using the GPT-Neo model.

The function `stop_server` uses the`zmq package` to listen for a stop signal to gracefully shutdown the Flask app.

Finally, the stop_thread is started as a background thread to listen for the stop signal. The Flask app is run on port `5000` with debug mode enabled.
## HTML template
This file is under templates/pages. The template contains a form with a text input field for users to enter their questions, and a submit button to send the question to the server. The form's action attribute is set to "/qa", indicating that the form data should be submitted to the server's "/qa" route. The template also includes a conditional statement to display the answer returned by the server if it exists. If the answer variable is not null, a paragraph element will be created with the answer text. The template's title, header, and labels provide clear instructions and guidance to the user on how to use the chatbot.
## Setup
1. Clone this repository to your local machine.
2. Create a virtual environment and activate it:
```
$ python3 -m venv venv
$ source venv/bin/activate
```
3. Install the dependencies:
```
$ pip install -r requirements.txt
```
## Usage
1. Run the Flask app:
```
$ export FLASK_APP=app.py
$ flask run
```
Or
```
$ python app.py
```
2. Open your web browser and navigate to `http://localhost:5000/` to see the chatbot interface.
3. Enter your question and click the "Ask" button to get an answer.
## Files
- `app.py`: The Flask application that serves the chatbot interface and handles questions.
- `index.html`: The HTML template for the chatbot interface.
- `requirements.txt`: A list of Python dependencies required to run the app (Not all of them required but was running more experiments on my repo).
- `README.md`: This file.

## Fine-tunning the model with Deepspeed and a dual RTX 6000
Deepspeed is an optimization library that provides an efficient way to fine-tune large language models (LLMs) like GPT-3 or T-NLG. The primary reasons to use Deepspeed for fine-tuning LLMs include its capability to significantly reduce the computational resources required, and its capacity to handle models that don't fit in a single GPU memory.
Deepspeed provides ZeRO (Zero Redundancy Optimizer), a memory optimization technology, which allows training models with billions of parameters without running out of memory. This is particularly useful when dealing with LLMs, which often have an enormous number of parameters.

Additionally, Deepspeed allows distributed training across multiple GPUs, which can considerably speed up the process and makes it possible to train much larger models than would be possible on a single GPU.

I have a simple Jupyter notebook. Let's first clone Deepspeed
```
$ git clone https://github.com/microsoft/DeepSpeed -b v0.4.0
$ cd DeepSpeed
```
Then install Deepspeed
```
$ pip install deepspeed
```
If you're going to use DeepSpeed's ZeRO functionality, you should also install NVIDIA's NCCL library, which is used for multi-GPU/multi-node communications.
Now let's clone the fine tuning repo
```
$ git clone https://github.com/Xirider/finetune-gpt2xl
```
Access the folder and download the datasets library
```
$ cd finetune-gpt2xl
$ pip install datasets
```
## Acknowledgments
This project was inspired by the GPT-3 Playground by OpenAI. The GPT-Neo model was trained by EleutherAI.
